# -*- coding: utf-8 -*-
"""DeBERTa_DML_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/137XHVafbMTUmUf_R3iM-2Ycup5wskaq2

### Setup (pip, imports)
!pip install -U datasets
!pip install pytorch-metric-learning
!apt install libomp-dev
!pip install faiss-cpu
"""
import torch
import torch.optim as optim
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel, DataCollatorForTokenClassification
from datasets import load_dataset, get_dataloaders
from pytorch_metric_learning import distances, losses, miners, reducers, testers
from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator
from datasets_utils import load_and_preprocess_dataset, unpack_dataset_info
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import f1_score

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

def tokenize_and_align_labels(examples):
    tokenized_inputs = hf_tokenizer(examples["tokens"], truncation=True, padding='longest',
                                    is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"{task}_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

def collate_fn(batch):
    # Separate inputs and labels
    input_ids = [item["input_ids"] for item in batch]
    #print("input_ids.shape:", input_ids.shape)
    attention_mask = [item["attention_mask"] for item in batch]
    #print("attention_mask.shape:", attention_mask)
    labels = [item["labels"] for item in batch]
    #print("labels:", labels.shape)

    # Pad sequences to the longest in the batch
    padded_inputs = hf_tokenizer.pad(
        {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels},
        padding="longest", return_tensors="pt"
    )

    return padded_inputs

def reshape_embeddings_and_labels(embeddings, labels):
    # Make labels 1D
    labels_reshaped = labels.reshape(-1)
    # Mask out -100 labels
    mask = labels_reshaped != -100
    labels_masked = labels_reshaped[mask]
    # Reshape embeddings to [tokens, embedding_dim]
    emb_reshaped = embeddings.reshape(-1, embeddings.shape[-1])
    # Mask embeddings of -100 labels
    emb_reshaped_masked = emb_reshaped[mask]
    """
    print(f"emb_reshaped.shape: {emb_reshaped.shape}")
    print(f"emb_reshaped_masked.shape: {emb_reshaped_masked.shape}")
    print(f"labels_reshaped.shape: {labels_reshaped.shape}")
    print(f"labels_masked.shaped\n: {labels_masked.shape}")
    """
    return emb_reshaped_masked, labels_masked


def forward_one_epoch(model, train_loader, optimizer, loss_func, mining_func, device):
    model.train()
    total_loss = 0.0
    num_batches = 0
    for batch_idx, data in enumerate(train_loader):
        input_ids = data['input_ids'].to(device)
        attention_mask = data['attention_mask'].to(device)
        labels = data['labels'].to(device)
        optimizer.zero_grad()
        embeddings = hf_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        emb_masked, labels_masked = reshape_embeddings_and_labels(embeddings, labels)
        indices_tuple = mining_func(emb_masked, labels_masked)
        loss = loss_func(emb_masked, labels_masked, indices_tuple)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        num_batches += 1
        if batch_idx % PRINT_FREQ == 0:
            print(
                "Epoch {} Iteration {}: curr_mean_loss = {}, Number of mined triplets = {}".format(
                    epoch, batch_idx, total_loss / num_batches, mining_func.num_triplets
                )
            )
    mean_loss = total_loss / num_batches if num_batches > 0 else 0.0
    return mean_loss

def train(config):
    EPOCHS = config['num_epochs']
    for dataset in config['datasets']:
        # Tokenizer and Model
        hf_tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")
        hf_model = AutoModel.from_pretrained("microsoft/deberta-v3-base")
        print("hf_model:", hf_model)
        # Load and preprocess the dataset
        ds_info_dict, train_ds_name, valid_ds_name, n_labels = unpack_dataset_info(dataset_name)
        tokenized_aligned_dataset, labels_list, id2label, label2id = load_and_preprocess_dataset(ds_info_dict, hf_tokenizer, config)
        train_loader, val_loader = get_dataloaders(tokenized_aligned_dataset, config, 
                                                    ["train", "val"], collate_fn=collate_fn)
    
        hf_model = hf_model.to(device)
        optimizer = optim.Adam(model.parameters(), lr=LR)

        ### pytorch-metric-learning stuff ###
        distance = distances.CosineSimilarity()
        reducer = reducers.ThresholdReducer(low=0)
        loss_func = losses.TripletMarginLoss(margin=0.2, distance=distance, reducer=reducer)
        mining_func = miners.TripletMarginMiner(margin=0.2, distance=distance,
                                                type_of_triplets="semihard")
        accuracy_calculator = AccuracyCalculator(include=("precision_at_1",), k=1)
    
        print("Starting training...")
        train_loss, eval_loss, eval_prec = [], [], []
        best_prec = 0.0
        patience_counter = 0

        for epoch in range(1, EPOCHS + 1):
            epoch_avg_loss = forward_one_epoch(model, loss_func, mining_func, device, 
                            train_loader, optimizer, epoch)
            
            prec = test(train_dataset, eval_dataset, model, accuracy_calculator)
            
            train_loss.append(epoch_avg_loss)
            eval_prec.append(prec)
            print("Epoch {}: train_loss: {:.4f}, eval_loss: {:.4f}, eval_p_@1: {:.4f} ".format(
                epoch, train_loss[-1], eval_loss[-1], eval_prec[-1]))
            if prec > best_prec:
                best_prec = prec
                patience_counter = 0
                torch.save(model.state_dict(), "best_deberta_dml_model.pth")
                print(f"Saved best model with precision: {best_prec}")
            else:
                patience_counter += 1
                if patience_counter >= PATIENCE:
                    print(f"Early stopping at epoch {epoch}, no improvement in {PATIENCE} epochs.")
                    break
        test_prec = test(train_dataset, test_dataset, model, accuracy_calculator)

        print(f"Final test precision: {test_prec}")
        results_df = pd.DataFrame({
            "epoch": list(range(1, len(loss_train) + 1)),
            "train_loss": train_loss,
            "eval_precision_at_1": eval_prec
        })
        results_df.to_csv("training_results.csv", index=False)

def get_all_embeddings(dataset, model):
    loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)
    embeddings_lst = []
    labels_lst = []
    model.eval()
    with torch.no_grad():
        for batch_idx, data in enumerate(loader):
            input_ids = data['input_ids'].to(device)
            attention_mask = data['attention_mask'].to(device)
            labels = data['labels'].to(device)
            embeddings = hf_model(input_ids=input_ids, 
                                    attention_mask=attention_mask).last_hidden_state
            emb_masked, labels_masked = reshape_embeddings_and_labels(embeddings, labels)
            embeddings_lst.append(emb_masked)
            labels_lst.append(labels_masked)
    stacked_embeddings = torch.cat(embeddings_lst)
    stacked_labels = torch.cat(labels_lst)
    return stacked_embeddings, stacked_labels
### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###
def test(train_set, test_set, model, accuracy_calculator):
    train_embeddings, train_labels = get_all_embeddings(train_set, model)
    test_embeddings, test_labels = get_all_embeddings(test_set, model)
    print("embeddings extracted, calculating accuracy...")
    accuracies = accuracy_calculator.get_accuracy(
        test_embeddings, test_labels, train_embeddings, train_labels, False
    )
    return accuracies

def run_experiments(configs_paths_lst: list):
    for config_path in configs_paths_lst:
            with open(config_path, 'r') as file:
                config = json.load(file)
            train(config)

if __name__ == "__main__":
    # Run specific config file if user specifies it
    parser = argparse.ArgumentParser(description="Run Deep Metric Learning fine-tuning experiments.")
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Specify a config file in the dml/configs directory (e.g., deberta_v3_base_DML_config.json). If not set, runs all configs."
    )
    parser.add_argument(
        "--test",
        action='store_true',
        default=True,
        help="After fine-tuning, loads best model and generate test results."
    )
    args = parser.parse_args()
    config_files_lst = []
    # If config file specified, mount path and send it to run_experiments
    if args.config:
        print(f"Running config file: {args.config}")
        config_files_lst = [os.path.join("dml/configs/", args.config)]
    # If not specified, mount all config files' paths and send em all to run_experiments
    else:
        print("Running all config files in configs/ directory.")
        for f in os.listdir("dml/configs/"):
            if f.endswith(".json"):
                config_files_lst.append(os.path.join("configs/", f))
    
    print(f"configs to run: {config_files_lst}")
    run_experiments(config_files_lst)

"""
if __name__ == "__main__":
    ### Dataset
    raw_ds_hf = load_dataset("peluz/lener_br", trust_remote_code=True)
    tokenized_datasets = raw_ds_hf.map(tokenize_and_align_labels, batched=True,
                                    batch_size = BATCH_SIZE)
    torch_ds = tokenized_datasets.with_format("torch")
    train_dataset = torch_ds['train']#.select(range(160))
    eval_dataset = torch_ds['validation']#.select(range(160))
    test_dataset = torch_ds['test']#.select(range(160))

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,
                                            collate_fn=collate_fn)
    val_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=BATCH_SIZE,
                                            collate_fn=collate_fn)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,
                                            collate_fn=collate_fn)

    model = hf_model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=LR)

    ### pytorch-metric-learning stuff ###
    accuracy_calculator = AccuracyCalculator(avg_of_avgs=True, include=("precision_at_1", "AMI",
    "NMI", "mean_average_precision", "mean_reciprocal_rank"), k=1)
    
    model_to_load = "best_deberta_dml_model.pth"
    model.load_state_dict(torch.load(model_to_load, map_location=device))
    print(f"Loaded model: {model_to_load}")
    test_prec = test(train_dataset, test_dataset, model, accuracy_calculator)
    print(f"Final test precision: {test_prec}")

    import umap
    import matplotlib.pyplot as plt

    # Get embeddings and labels (for example, from the train set)
    embeddings, labels = get_all_embeddings(test_dataset, model)
    embeddings_np = embeddings.cpu().numpy()
    labels_np = labels.cpu().numpy()

    # Fit UMAP
    umap_2d = umap.UMAP(n_components=2, random_state=42)
    embeddings_umap = umap_2d.fit_transform(embeddings_np)

    # Plot
    print("Plotting UMAP...")
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(embeddings_umap[:, 0], embeddings_umap[:, 1], c=labels_np, cmap='tab20', s=5, alpha=0.7)
    plt.colorbar(scatter, label='Label')
    plt.title("UMAP projection of test token embeddings")
    plt.xlabel("UMAP-1")
    plt.ylabel("UMAP-2")
    plt.tight_layout()
    plt.savefig("umap_test_embeddings.png", dpi=300)
    plt.close()
    print("Saved UMAP plot to umap_embeddings.png")
    

"""
